#!/usr/bin/env python3

"""
Initial venv setup:

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
import csv
import logging
from multiprocessing import cpu_count
from pathlib import Path
import os
import re
import shutil
import sys
import time
from traceback import print_tb
from urllib.parse import urljoin, unquote_plus
import zipfile

import requests
from requests_html import HTML


BASE_URL = 'https://www.jaylen.com.ar'
CATEGORIES_PATH = '/categorias'
TIMEOUT = 5
SESSION = requests.Session()
MAX_RETRIES = 3
NON_WORD_RE = re.compile('\W+')
SCRIPT_PATH = Path(__file__).resolve()
OUT_PATH = SCRIPT_PATH.parent / 'categories'
OUT_PATH.mkdir(exist_ok=True)
HEADERS = ['sku', 'description', 'price', 'img_url', 'thumb_url']
PRICE_RE = re.compile('\d+')
IS_WIN_PLAT = sys.platform.startswith('win')
CSV_DIALECT = 'excel' if IS_WIN_PLAT else 'unix'
ZIP_COMPRESSION = zipfile.ZIP_STORED if IS_WIN_PLAT else zipfile.ZIP_LZMA
SKU_RE = re.compile('\(([^)]+)\)\Z')
PRODS_PER_PAGE = 9
CPU_COUNT = cpu_count()

log_level = logging.DEBUG if os.environ.get('DEBUG') else logging.INFO
logging.basicConfig(filename=f'{SCRIPT_PATH.stem}.log', filemode='w',
                    level=log_level)

def get(url, params={}):
    logging.debug(f'GET {url}')
    retries = 0
    txt = None

    while txt is None and retries < MAX_RETRIES:
        try:
            res = SESSION.get(url, params=params, timeout=TIMEOUT)
        except requests.exceptions.Timeout:
            retries += 1
            logging.warning(f'GET {url} timed out [retry #{retries}]. Going to sleep for {retries} sec(s)')
            time.sleep(retries)
        else:
            if res.status_code == requests.codes.ok:
                txt = res.text

    return txt


def normalize_bname(basename):
    return NON_WORD_RE.sub('_', basename).casefold()


def write_cat_csv_file(doc, csv_path, page):
    rows = 0
    csv_open_mode = 'w' if page == 1 else 'a'

    with csv_path.open(mode=csv_open_mode, newline='') as cat_file:
        cat_writer = csv.writer(cat_file, dialect=CSV_DIALECT)
        if page == 1: cat_writer.writerow(HEADERS)

        for thumb in doc.find('.thumbnail'):
            img = thumb.find('img.product-thumb', first=True)
            thumb_url = urljoin(BASE_URL, img.attrs['src'])
            desc = img.attrs['alt']
            caption = thumb.find('.caption', first=True)
            # document.querySelector('.caption').querySelector('a[href^="mailto:"]')
            mailto_anchor = caption.find('a[href^="mailto:"]', first=True)
            sku = SKU_RE.search(unquote_plus(mailto_anchor.attrs['href'])).group(1)
            price_node = caption.find('.product-price', first=True)
            mo = PRICE_RE.match(price_node.text)
            price =  mo.group() if mo else None
            sku_node = thumb.find('[data-sku]', first=True)
            img_url = urljoin(BASE_URL, sku_node.attrs['data-image-url']) if sku_node else None

            row = [sku, desc, price, img_url, thumb_url]
            cat_writer.writerow(row)
            rows += 1

    return rows


def scrape_prods_from(url, *, category, parent_category=None):
    logging.debug(f'Scraping products from category: {category} (parent cat: #{parent_category})')
    csv_paths = set()
    page = 1
    done = False

    while not done:
        html = get(url, params={'pagina': page})
        if html is None: break

        doc = HTML(html=html)
        cat_bname = normalize_bname(category)
        if parent_category:
            par_cat_path = OUT_PATH / normalize_bname(parent_category)
            par_cat_path.mkdir(exist_ok=True)
            cat_path = par_cat_path / cat_bname
        else:
            cat_path = OUT_PATH / cat_bname

        cat_csv_path = cat_path.with_suffix('.csv')

        product_count = write_cat_csv_file(doc, cat_csv_path, page)
        if product_count and page == 1:
            csv_paths.add(cat_csv_path)

        done = product_count < PRODS_PER_PAGE
        if not done: page += 1

        if not parent_category:
            sub_cats = doc.find('a.list-group-item.subcategory-link')

            if sub_cats:
                with ThreadPoolExecutor(max_workers=CPU_COUNT) as executor:
                    futures = [executor.submit(scrape_prods_from,
                                               urljoin(BASE_URL, cat.attrs["href"]),
                                               category=cat.text,
                                               parent_category=category)
                               for cat in sub_cats]

                for future in as_completed(futures):
                    try:
                        sub_cat_csv_paths = future.result()
                    except:
                        exc_type, exc_val, exc_tb = sys.exc_info()
                        logging.error(str(exc_type))
                        print_tb(exc_tb)
                    else:
                        csv_paths.update(sub_cat_csv_paths)

                # sub_cat_urls = [urljoin(BASE_URL, cat.attrs["href"]) for cat in sub_cats]
                # sub_cat_names = [cat.text for cat in sub_cats]
                # sub_cat_cnt = len(sub_cats)
                # cat_names = repeat(category, times=sub_cat_cnt)
                # executors = repeat(None, times=sub_cat_cnt)
                # args = [sub_cat_urls, sub_cat_names, cat_names, executors]

                # for sub_cat_csv_paths in executor.map(scrape_prods_from, args):
                #     csv_paths.extend(sub_cat_csv_paths)

    return csv_paths


categories_url = urljoin(BASE_URL, CATEGORIES_PATH)
cats_html = get(categories_url)
if cats_html is None:
    sys.exit(f'Unable to scrape {categories_url}')

cats_doc = HTML(html=cats_html)
root_cats = cats_doc.find('span.label.label-default')
thr_cnt = min(CPU_COUNT*2, len(root_cats))

zip_path = OUT_PATH.with_suffix('.zip')
with zipfile.ZipFile(zip_path, mode='w', compresslevel=ZIP_COMPRESSION) as zip:
    with ThreadPoolExecutor(max_workers=thr_cnt) as executor:
        # document.querySelectorAll('span.label.label-default').length
        futures = [executor.submit(scrape_prods_from,
                                   urljoin(f'{categories_url}/', root_cat.text),
                                   category=root_cat.text)
                   for root_cat in root_cats]

        for future in as_completed(futures):
            try:
                csv_paths = future.result()
            except:
                exc_type, exc_val, exc_tb = sys.exc_info()
                logging.error(str(exc_type))
                print_tb(exc_tb)
            else:
                for csv_path in csv_paths:
                    zip.write(csv_path, arcname=csv_path.relative_to(OUT_PATH))

shutil.rmtree(OUT_PATH)
