#!/usr/bin/env python3

"""
Initial venv setup:

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
import csv
import logging
from multiprocessing import cpu_count
from pathlib import Path
import os
import re
import shutil
import sys
import time
from traceback import print_tb
from urllib.parse import urljoin, unquote_plus
import zipfile

from bs4 import BeautifulSoup
import requests


BASE_URL = 'https://www.jaylen.com.ar'
CATEGORIES_PATH = '/categorias'
TIMEOUT = int(os.environ.get('TIMEOUT', 5))
SESSION = requests.Session()
MAX_RETRIES = 3
NON_WORD_RE = re.compile('\W+')
SCRIPT_PATH = Path(__file__).resolve()
OUT_PATH = SCRIPT_PATH.parent / 'categories'
OUT_PATH.mkdir(exist_ok=True)
HEADERS = ['sku', 'description', 'price', 'img_url', 'thumb_url']
PRICE_RE = re.compile('\d+')
IS_WIN_PLAT = sys.platform.startswith('win')
CSV_DIALECT = 'excel' if IS_WIN_PLAT else 'unix'
ZIP_COMPRESSION = zipfile.ZIP_STORED if IS_WIN_PLAT else zipfile.ZIP_LZMA
SKU_RE = re.compile('\(([^)]+)\)\Z')
PRODS_PER_PAGE = 9
CPU_COUNT = cpu_count()
HTML_PARSER = 'html.parser'

log_level = logging.DEBUG if os.environ.get('DEBUG') else logging.INFO
logging.basicConfig(filename=f'{SCRIPT_PATH.stem}.log', filemode='w',
                    level=log_level)

def get(url, params={}):
    logging.debug(f'GET {url}')
    retries = 0
    txt = None

    while txt is None and retries < MAX_RETRIES:
        try:
            res = SESSION.get(url, params=params, timeout=TIMEOUT)
        except requests.exceptions.Timeout:
            retries += 1
            logging.warning(f'GET {url} timed out [retry #{retries}]. Going to sleep for {retries} sec(s)')
            time.sleep(retries)
        else:
            if res.status_code == requests.codes.ok:
                txt = res.text

    return txt


def normalize_bname(basename):
    return NON_WORD_RE.sub('_', basename).casefold()


def write_cat_csv_file(doc, csv_path, page):
    rows = 0
    csv_open_mode = 'w' if page == 1 else 'a'

    with csv_path.open(mode=csv_open_mode, newline='') as cat_file:
        cat_writer = csv.writer(cat_file, dialect=CSV_DIALECT)
        if page == 1: cat_writer.writerow(HEADERS)

        for thumb in doc.select('.thumbnail'):
            img = thumb.find('img', class_='product-thumb')
            thumb_url = urljoin(BASE_URL, img['src'])
            desc = img['alt']
            caption = thumb.select('.caption', limit=1)[0]
            # document.querySelector('.caption').querySelector('a[href^="mailto:"]')
            mailto_anchor = caption.select('a[href^="mailto:"]', limit=1)[0]
            sku = SKU_RE.search(unquote_plus(mailto_anchor['href'])).group(1)
            price_node = caption.select('.product-price', limit=1)[0]
            mo = PRICE_RE.match(price_node.string)
            price =  mo.group() if mo else None
            sku_node = thumb.find(attrs={'data-sku': True})
            img_url = urljoin(BASE_URL, sku_node['data-image-url']) if sku_node else None

            row = [sku, desc, price, img_url, thumb_url]
            cat_writer.writerow(row)
            rows += 1

    return rows


def scrape_prods_from(url, *, category, parent_category=None):
    csv_paths = set()
    page = 1
    done = False

    while not done:
        html = get(url, params={'pagina': page})
        if html is None: break

        doc = BeautifulSoup(html, HTML_PARSER)
        cat_bname = normalize_bname(category)
        if parent_category:
            par_cat_path = OUT_PATH / normalize_bname(parent_category)
            par_cat_path.mkdir(exist_ok=True)
            cat_path = par_cat_path / cat_bname
        else:
            cat_path = OUT_PATH / cat_bname

        cat_csv_path = cat_path.with_suffix('.csv')

        product_count = write_cat_csv_file(doc, cat_csv_path, page)
        if product_count and page == 1:
            csv_paths.add(cat_csv_path)

        done = product_count < PRODS_PER_PAGE
        if not done: page += 1

        if not parent_category:
            sub_cats = doc.select('a.list-group-item.subcategory-link')

            if sub_cats:
                with ThreadPoolExecutor(max_workers=CPU_COUNT) as executor:
                    futures = [executor.submit(scrape_prods_from,
                                               urljoin(BASE_URL, cat["href"]),
                                               category=cat.string,
                                               parent_category=category)
                               for cat in sub_cats]

                for future in as_completed(futures):
                    try:
                        sub_cat_csv_paths = future.result()
                    except:
                        logging.exception(f"Error scraping a subcat of root cat: '{category}'")
                    else:
                        csv_paths.update(sub_cat_csv_paths)

    return csv_paths


categories_url = urljoin(BASE_URL, CATEGORIES_PATH)
cats_html = get(categories_url)
if cats_html is None:
    sys.exit(f'Unable to scrape {categories_url}')

cats_doc = BeautifulSoup(cats_html, HTML_PARSER)
# document.querySelectorAll('span.label.label-default').length
root_cats = cats_doc.select('span.label.label-default')
thr_cnt = min(CPU_COUNT, len(root_cats))

zip_path = OUT_PATH.with_suffix('.zip')
with zipfile.ZipFile(zip_path, mode='w', compresslevel=ZIP_COMPRESSION) as zip:
    with ThreadPoolExecutor(max_workers=thr_cnt) as executor:
        futures = [executor.submit(scrape_prods_from,
                                   urljoin(f'{categories_url}/', root_cat.string),
                                   category=root_cat.string)
                   for root_cat in root_cats]

        for future in as_completed(futures):
            try:
                csv_paths = future.result()
            except:
                logging.exception(f"Error scraping a root category")
            else:
                for csv_path in csv_paths:
                    zip.write(csv_path, arcname=csv_path.relative_to(OUT_PATH))

shutil.rmtree(OUT_PATH)
