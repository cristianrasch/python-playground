#!/usr/bin/env python3

"""
Initial venv setup:

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
"""

import csv
import logging
from pathlib import Path
import os
import re
import shutil
import sys
import time
from urllib.parse import urljoin, unquote_plus
import zipfile

import requests
from requests_html import HTML


BASE_URL = 'https://www.jaylen.com.ar'
CATEGORIES_PATH = '/categorias'
TIMEOUT = 5
SESSION = requests.Session()
MAX_RETRIES = 3
NON_WORD_RE = re.compile('\W+')
SCRIPT_PATH = Path(__file__).resolve()
OUT_PATH = SCRIPT_PATH.parent / 'categories'
OUT_PATH.mkdir(exist_ok=True)
HEADERS = ['sku', 'description', 'price', 'img_url', 'thumb_url']
PRICE_RE = re.compile('\d+')
IS_WIN_PLAT = sys.platform.startswith('win')
CSV_DIALECT = 'excel' if IS_WIN_PLAT else 'unix'
ZIP_COMPRESSION = zipfile.ZIP_STORED if IS_WIN_PLAT else zipfile.ZIP_LZMA
SKU_RE = re.compile('\(([^)]+)\)\Z')
PRODS_PER_PAGE = 9
CSV_PATHS = set()

log_level = logging.DEBUG if os.environ.get('DEBUG') else logging.INFO
logging.basicConfig(filename=f'{SCRIPT_PATH.stem}.log', filemode='w',
                    level=log_level)

def get(url, params={}):
    logging.debug(f'GET {url}')
    retries = 0
    txt = None

    while txt is None and retries < MAX_RETRIES:
        try:
            res = SESSION.get(url, params=params, timeout=TIMEOUT)
        except requests.exceptions.Timeout:
            retries += 1
            logging.warning(f'GET {url} timed out [retry #{retries}]. Going to sleep for {retries} sec(s)')
            time.sleep(retries)
        else:
            if res.status_code == requests.codes.ok:
                txt = res.text

    return txt


def normalize_bname(basename):
    return NON_WORD_RE.sub('_', basename).casefold()


def write_cat_csv_file(doc, csv_path, page):
    rows = 0
    csv_open_mode = 'w' if page == 1 else 'a'

    with csv_path.open(mode=csv_open_mode, newline='') as cat_file:
        cat_writer = csv.writer(cat_file, dialect=CSV_DIALECT)
        if page == 1: cat_writer.writerow(HEADERS)

        for thumb in doc.find('.thumbnail'):
            img = thumb.find('img.product-thumb', first=True)
            thumb_url = urljoin(BASE_URL, img.attrs['src'])
            desc = img.attrs['alt']
            caption = thumb.find('.caption', first=True)
            # document.querySelector('.caption').querySelector('a[href^="mailto:"]')
            mailto_anchor = caption.find('a[href^="mailto:"]', first=True)
            sku = SKU_RE.search(unquote_plus(mailto_anchor.attrs['href'])).group(1)
            price_node = caption.find('.product-price', first=True)
            mo = PRICE_RE.match(price_node.text)
            price =  mo.group() if mo else None
            sku_node = thumb.find('[data-sku]', first=True)
            img_url = urljoin(BASE_URL, sku_node.attrs['data-image-url']) if sku_node else None

            row = [sku, desc, price, img_url, thumb_url]
            cat_writer.writerow(row)
            rows += 1

    return rows


def scrape_prods_from(url, category, *, parent_category=None):
    page = 1
    done = False

    while not done:
        html = get(url, params={'pagina': page})
        if html is None: break

        doc = HTML(html=html)
        cat_bname = normalize_bname(category)
        if parent_category:
            par_cat_path = OUT_PATH / normalize_bname(parent_category)
            par_cat_path.mkdir(exist_ok=True)
            cat_path = par_cat_path / cat_bname
        else:
            cat_path = OUT_PATH / cat_bname

        cat_csv_path = cat_path.with_suffix('.csv')

        product_count = write_cat_csv_file(doc, cat_csv_path, page)
        if product_count and page == 1:
            CSV_PATHS.add(cat_csv_path)

        done = product_count < PRODS_PER_PAGE
        if not done: page += 1

        if not parent_category:
            sub_cats = doc.find('a.list-group-item.subcategory-link')

            if sub_cats:
                for sub_cat in sub_cats:
                    sub_cat_url = urljoin(BASE_URL, sub_cat.attrs["href"])
                    scrape_prods_from(sub_cat_url, category=sub_cat.text,
                                      parent_category=category)


categories_url = urljoin(BASE_URL, CATEGORIES_PATH)
cats_html = get(categories_url)
if cats_html is None:
    sys.exit(f'Unable to scrape {categories_url}')

cats_doc = HTML(html=cats_html)
# document.querySelectorAll('span.label.label-default').length
for root_cat in cats_doc.find('span.label.label-default'):
    root_cat_url = urljoin(f'{categories_url}/', root_cat.text)
    scrape_prods_from(root_cat_url, category=root_cat.text)


zip_path = OUT_PATH.with_suffix('.zip')
with zipfile.ZipFile(zip_path, mode='w', compresslevel=ZIP_COMPRESSION) as zip:
    for csv_path in CSV_PATHS:
        zip.write(csv_path, arcname=csv_path.relative_to(OUT_PATH))
shutil.rmtree(OUT_PATH)
